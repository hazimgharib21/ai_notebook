{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "from PIL import Image\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "src = 'dataset/PetImages/'\n",
    "\n",
    "# Check if the dataset has been downloaded. If not, direct user to download the dataset first\n",
    "if not os.path.isdir(src):\n",
    "    print(\"\"\"\n",
    "          Dataset not found in your computer.\n",
    "          Please follow the instructions in the link below to download the dataset:\n",
    "          https://raw.githubusercontent.com/PacktPublishing/Neural-Network-Projects-with-Python/master/chapter4/how_to_download_the_dataset.txt\n",
    "          \"\"\")\n",
    "    quit()\n",
    "\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_test_split(src_folder, train_size = 0.8):\n",
    "    # Make sure we remove any existing folders and start from a clean slate\n",
    "    shutil.rmtree(src_folder+'Train/Cat/', ignore_errors=True)\n",
    "    shutil.rmtree(src_folder+'Train/Dog/', ignore_errors=True)\n",
    "    shutil.rmtree(src_folder+'Test/Cat/', ignore_errors=True)\n",
    "    shutil.rmtree(src_folder+'Test/Dog/', ignore_errors=True)\n",
    "\n",
    "    # Now, create new empty train and test folders\n",
    "    os.makedirs(src_folder+'Train/Cat/')\n",
    "    os.makedirs(src_folder+'Train/Dog/')\n",
    "    os.makedirs(src_folder+'Test/Cat/')\n",
    "    os.makedirs(src_folder+'Test/Dog/')\n",
    "\n",
    "    # Get the number of cats and dogs images\n",
    "    _, _, cat_images = next(os.walk(src_folder+'Cat/'))\n",
    "    files_to_be_removed = ['Thumbs.db', '666.jpg', '835.jpg']\n",
    "    for file in files_to_be_removed:\n",
    "        cat_images.remove(file)\n",
    "    num_cat_images = len(cat_images)\n",
    "    num_cat_images_train = int(train_size * num_cat_images)\n",
    "    num_cat_images_test = num_cat_images - num_cat_images_train\n",
    "\n",
    "    _, _, dog_images = next(os.walk(src_folder+'Dog/'))\n",
    "    files_to_be_removed = ['Thumbs.db', '11702.jpg']\n",
    "    for file in files_to_be_removed:\n",
    "        dog_images.remove(file)\n",
    "    num_dog_images = len(dog_images)\n",
    "    num_dog_images_train = int(train_size * num_dog_images)\n",
    "    num_dog_images_test = num_dog_images - num_dog_images_train\n",
    "\n",
    "    # Randomly assign images to train and test\n",
    "    cat_train_images = random.sample(cat_images, num_cat_images_train)\n",
    "    for img in cat_train_images:\n",
    "        shutil.copy(src=src_folder+'Cat/'+img, dst=src_folder+'Train/Cat/')\n",
    "    cat_test_images  = [img for img in cat_images if img not in cat_train_images]\n",
    "    for img in cat_test_images:\n",
    "        shutil.copy(src=src_folder+'Cat/'+img, dst=src_folder+'Test/Cat/')\n",
    "\n",
    "    dog_train_images = random.sample(dog_images, num_dog_images_train)\n",
    "    for img in dog_train_images:\n",
    "        shutil.copy(src=src_folder+'Dog/'+img, dst=src_folder+'Train/Dog/')\n",
    "    dog_test_images  = [img for img in dog_images if img not in dog_train_images]\n",
    "    for img in dog_test_images:\n",
    "        shutil.copy(src=src_folder+'Dog/'+img, dst=src_folder+'Test/Dog/')\n",
    "\n",
    "    # remove corrupted exif data from the dataset\n",
    "    remove_exif_data(src_folder+'Train/')\n",
    "    remove_exif_data(src_folder+'Test/')\n",
    "\n",
    "# helper function to remove corrupt exif data from Microsoft's dataset\n",
    "def remove_exif_data(src_folder):\n",
    "    _, _, cat_images = next(os.walk(src_folder+'Cat/'))\n",
    "    for img in cat_images:\n",
    "        try:\n",
    "            imag = Image.open(src_folder+'Cat/'+img)\n",
    "            exif_data = imag._getexif()\n",
    "        except ValueError as err:\n",
    "            print(err)\n",
    "\n",
    "    _, _, dog_images = next(os.walk(src_folder+'Dog/'))\n",
    "    for img in dog_images:\n",
    "        try:\n",
    "            imag = Image.open(src_folder+'Dog/'+img)\n",
    "            exif_data = imag._getexif()\n",
    "        except ValueError as err:\n",
    "            print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the train/test folders if it does not exists already\n",
    "if not os.path.isdir(src+'train/'):\n",
    "    train_test_split(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\invader\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58892288/58889256 [==============================] - 715s 12us/step\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameters\n",
    "INPUT_SIZE = 48 #Change this to 48 if the code is taking too long to run\n",
    "BATCH_SIZE = 16\n",
    "STEPS_PER_EPOCH = 200\n",
    "EPOCHS = 3\n",
    "\n",
    "vgg16 = VGG16(include_top=False, weights='imagenet', input_shape=(INPUT_SIZE,INPUT_SIZE,3))\n",
    "\n",
    "# Freeze the pre-trained layers\n",
    "for layer in vgg16.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19997 images belonging to 2 classes.\n",
      "Found 5000 images belonging to 2 classes.\n",
      "WARNING:tensorflow:From C:\\Users\\invader\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/3\n",
      "200/200 [==============================] - 551s 3s/step - loss: 0.6269 - acc: 0.6384\n",
      "Epoch 2/3\n",
      "200/200 [==============================] - 472s 2s/step - loss: 0.5696 - acc: 0.7109\n",
      "Epoch 3/3\n",
      "200/200 [==============================] - 466s 2s/step - loss: 0.5409 - acc: 0.7262\n",
      "loss: 0.5532941031455993\n",
      "acc: 0.70125\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Add a fully connected layer with 1 node at the end \n",
    "input_ = vgg16.input\n",
    "output_ = vgg16(input_)\n",
    "last_layer = Flatten(name='flatten')(output_)\n",
    "last_layer = Dense(1, activation='sigmoid')(last_layer)\n",
    "model = Model(input=input_, output=last_layer)\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "training_data_generator = ImageDataGenerator(rescale = 1./255)\n",
    "testing_data_generator = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "training_set = training_data_generator.flow_from_directory(src+'Train/',\n",
    "                                                target_size = (INPUT_SIZE, INPUT_SIZE),\n",
    "                                                batch_size = BATCH_SIZE,\n",
    "                                                class_mode = 'binary')\n",
    "\n",
    "test_set = testing_data_generator.flow_from_directory(src+'Test/',\n",
    "                                             target_size = (INPUT_SIZE, INPUT_SIZE),\n",
    "                                             batch_size = BATCH_SIZE,\n",
    "                                             class_mode = 'binary')\n",
    "\n",
    "model.fit_generator(training_set, steps_per_epoch = STEPS_PER_EPOCH, epochs = EPOCHS, verbose=1)\n",
    "\n",
    "score = model.evaluate_generator(test_set, steps=100)\n",
    "\n",
    "for idx, metric in enumerate(model.metrics_names):\n",
    "    print(\"{}: {}\".format(metric, score[idx]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
